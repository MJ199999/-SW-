{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJ199999/SW-Project/blob/master/text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "TNHM62bn1_1K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1d5c27-64f4-4964-9be5-64af098a6c64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.21.2-py3-none-any.whl (4.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.7 MB 7.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 58.1 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 78.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.9.1 tokenizers-0.12.1 transformers-4.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PRb_n97i_dU0",
        "outputId": "1a5e89fa-5d4e-405e-ba19-642d00910138"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyALBUM-0vQB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from transformers import GPT2LMHeadModel, AutoModel\n",
        "\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description=\"Generate a text\"\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"\",\n",
        "        type=str,\n",
        "        default=None,\n",
        "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
        "    )\n",
        "    parser.add_argument(\n",
        "        \"--prompt_text\",\n",
        "        type=str,\n",
        "        default=\"함께\",\n",
        "        help=\"Prompting text\",\n",
        "    )\n",
        "\n",
        "    # Sanity checks\n",
        "    if args.model_name_or_path is None:\n",
        "        raise ValueError(\"Need model name or path.\")\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def main():\n",
        "    args = parse_args()\n",
        "\n",
        "    model_path = args.model_name_or_path\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForCausalLM.from_pretrained(model_path)\n",
        "    text = args.prompt_text\n",
        "    input_ids = tokenizer.encode(text)\n",
        "    # Check generation time\n",
        "    start = time.time() \n",
        "    gen_ids = model.generate(torch.tensor([input_ids]),\n",
        "                            max_length=1024,\n",
        "                            repetition_penalty=2.0,\n",
        "                            pad_token_id=tokenizer.pad_token_id,\n",
        "                            eos_token_id=tokenizer.eos_token_id,\n",
        "                            bos_token_id=tokenizer.bos_token_id,\n",
        "                            # num_beams=5,\n",
        "                            do_sample=True,\n",
        "                            top_k=30, \n",
        "                            top_p=0.95,\n",
        "                            use_cache=True)\n",
        "    generated_text = tokenizer.decode(gen_ids[0,:].tolist())\n",
        "    end = time.time()\n",
        "    \n",
        "    print(generated_text)\n",
        "    print(f\"{end - start:.5f} sec\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModelForCausalLM\n",
        "\n",
        "model = TFAutoModelForCausalLM.from_pretrained(\"/gdrive/MyDrive/Colab Notebooks/sw-project/SW-Project/skt-ko-gpt-trinity-1.2B-v0.5-2\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S9gh0XrH6YyM",
        "outputId": "e5878a71-e48b-407e-ad83-6969d2934120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "All model checkpoint layers were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the layers of TFGPT2LMHeadModel were initialized from the model checkpoint at /gdrive/MyDrive/Colab Notebooks/sw-project/SW-Project/skt-ko-gpt-trinity-1.2B-v0.5-2.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_key_len = 5"
      ],
      "metadata": {
        "id": "fIuPw4N0ESZu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"skt/ko-gpt-trinity-1.2B-v0.5\", bos_token='</s>', eos_token='</s>', pad_token='<pad>')\n",
        "keyword = \"사랑\"\n",
        "\n",
        "bos_token = [tokenizer.bos_token]\n",
        "eos_token = [tokenizer.eos_token]\n",
        "\n",
        "tokens = bos_token + tokenizer.tokenize(keyword) + eos_token\n",
        "input_id = tokenizer.convert_tokens_to_ids(tokens)\n",
        "input_id = pad_sequences([input_id], maxlen=max_key_len, value=tokenizer.pad_token_id, padding='post')[0]\n",
        "\n",
        "input_id = np.array(input_id, dtype=int)"
      ],
      "metadata": {
        "id": "Hy6HzLFV4tIV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZycMPCUqaWi_",
        "outputId": "d0ac23ee-2a09-468a-b294-3a0763294c8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([    1, 30716,     1,     3,     3])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids1 = tokenizer(keyword, return_tensors='pt').input_ids"
      ],
      "metadata": {
        "id": "YKt8BArCaMeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7t0hLHOaUIn",
        "outputId": "15451c4e-cb4b-43c9-a08b-6927ba757982"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[30716]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids1.unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQheeZdVF7PA",
        "outputId": "075fa0fe-7a95-4bf5-ddf5-11f1b5f20632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[30716]]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_id = model.generate(input_ids1, do_sample=False, max_length=200, repetition_penalty=2.0)\n",
        "gen_text = tokenizer.batch_decode(gen_id, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "FGSJo4pW-1ee"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeC89uGEE1Za",
        "outputId": "c92e484d-fac2-4c45-de00-6c265990321e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['사랑시그이다.\\n다면눈고 바 불러나의\\n들 눈다.바저 \\n그리 바다에에 이름을나는동그는.\\n랴오는억신의다가부한\\t 만나가지지 와서에게오세 싶다.\\n르르 않는푸기 그의월은을넓저리스떨그러우리하리는들은만울 그하나누나밤보 있다.\\n흐라.\\n노 끝 바다 싶다. 나무날올깨려 없는여개 그대 있는인칠 어둠비물 너깔교']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "name": "text_generation.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyN37g8WXHyI0OZHRCNf6q4A",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}