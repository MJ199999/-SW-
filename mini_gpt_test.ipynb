{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MJ199999/SW-Project/blob/master/mini_gpt_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ0-ielXKbkP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import re\n",
        "import string\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kwpf1bwFLAl3",
        "outputId": "c5a2e1ab-b042-487e-dfdf-da373c30e99d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 당 평균 길이: 3.9882862644415917\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_excel(\"https://github.com/MJ199999/SW-Project/raw/master/poem_key.xlsx\")\n",
        "\n",
        "line_num = 0\n",
        "word_num = 0\n",
        "hist_word_num = []\n",
        "df_poem = df['시']\n",
        "for i in range(0, len(df_poem)):\n",
        "  poem = df_poem.iloc[i] # 시 하나 들고오기\n",
        "  line = poem.splitlines() # 그 시를 행 기준으로 나눠서 리스트로 저장\n",
        "  for j in range(0, len(line)): # 연 나누는 빈 줄 세기\n",
        "    stanza_num = 0\n",
        "    b = (line[j] == '')\n",
        "    stanza_num += b\n",
        "  line_num += (len(line)-stanza_num) # 행 개수를 센다. 연 간의 빈 라인까지 합하기 때문에 이거 빼줘야 함.\n",
        "  p_str = ' '.join(line) # line를 다시 문자열로 만들어준다.\n",
        "  word_num += len(p_str.split()) # 시의 전체 단어 개수 센다. \n",
        "  # 히스토그램 그리기 위해 계산 코드\n",
        "  for k in range(0, len(line)):\n",
        "    hist_word_num.append(len(line[k].split()))\n",
        "word_per_line = word_num/line_num # 단어 개수 / 행 개수 = 각 행당 평균 단어 개수\n",
        "print(\"문장 당 평균 길이:\", word_per_line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E8bP-v-ChR-y"
      },
      "outputs": [],
      "source": [
        "plt_word_num = pd.DataFrame(hist_word_num)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y36XfEVVn4zT"
      },
      "outputs": [],
      "source": [
        "plt_word_num = plt_word_num.rename(columns={0: '1번'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_vv0GDr0bcgQ",
        "outputId": "b6908f0d-12e7-4283-dcf7-32b1eabf253f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(0.5, 0, '행 당 단어 개수')"
            ]
          },
          "execution_count": 153,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 54665 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45817 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 50612 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 49688 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 54665 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45817 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 45800 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 50612 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 44060 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.7/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 49688 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4IAAANcCAYAAAAKPRE+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdeZTkd1nv8c9jhlUFAhkiJtFEQQQ9itwxol64XqJAwhIIAeMRiBDNFQFZXAjquXhdzgEBw6bhRAIE5LIlQQJGIBfB5VyJJggYiEpkkeRmGWVTOYrR7/2jfhM6w3R3VU0vM/O8Xuf0mdp+T3976tdV9e6qrq4xRgAAAOjjq7Z7AQAAAGwtIQgAANCMEAQAAGhGCAIAADQjBAEAAJrZsd0L2AxHHHHEOPbYY7d7GQAAANviiiuu+Icxxs7Vzj8kQ/DYY4/N5Zdfvt3LAAAA2BZV9am1zvfSUAAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoZsd2L4Ct8/cvO22p7b7haW/c4JUAAADbyTOCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmdmz3Aji4XPVbJy+13b2e8rYNXgkAALAszwgCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0s2khWFWvqqobq+rKFafduaouraqPTf8ePp1eVfXSqrq6qj5cVfddsc3p0+U/VlWnb9Z6AQAAutjMZwRfk+Qhe512VpL3jDHukeQ90/EkOTHJPaaPM5Ock8zCMclzk3xPkuOTPHdPPAIAALCcTQvBMcYfJ/nMXiefnOT86fD5SR654vTXjpn3J7lTVd0tyYOTXDrG+MwY47NJLs1XxiUAAAAL2OrfETxyjHHddPj6JEdOh49K8ukVl7tmOm21079CVZ1ZVZdX1eW7d+/e2FUDAAAcQrbtzWLGGCPJ2MB5544xdo0xdu3cuXOjxgIAABxytjoEb5he8pnp3xun069NcsyKyx09nbba6QAAACxpq0Pw4iR73vnz9CRvW3H6E6Z3D71fks9PLyF9V5IHVdXh05vEPGg6DQAAgCXt2KzBVfWGJD+Q5Iiquiazd/98XpI3V9UZST6V5LHTxS9JclKSq5N8MckTk2SM8Zmq+tUkfzFd7lfGGHu/AQ0AAAAL2LQQHGP8yCpnnbCPy44kT1llzquSvGoDlwYAANDatr1ZDAAAANtDCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgmR3bvQDmc91vn7XUdnf7qedt8EoAAICDnWcEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzO7Z7AZtp9zm/u9R2O5/8uA1eCQAAwIHDM4IAAADNCEEAAIBmhCAAAEAz2xKCVfXMqvpIVV1ZVW+oqttW1XFVdVlVXV1Vb6qqW0+Xvc10/Orp/GO3Y80AAACHii0Pwao6KslPJ9k1xvj2JIclOS3J85OcPca4e5LPJjlj2uSMJJ+dTj97uhwAAABL2q6Xhu5Icruq2pHk9kmuS/LAJBdM55+f5JHT4ZOn45nOP6GqagvXCgAAcEjZ8hAcY1yb5IVJ/j6zAPx8kiuSfG6McdN0sWuSHDUdPirJp6dtb5ouf5etXDMAAMChZDteGnp4Zs/yHZfk65N8dZKHbMDcM6vq8qq6fPfu3fs7DgAA4JC1HS8N/cEknxhj7B5j/HuSi5J8f5I7TS8VTZKjk1w7Hb42yTFJMp1/xyT/uPfQMca5Y4xdY4xdO3fu3OyvAQAA4KC1HSH490nuV1W3n37X74QkH03y3iSnTpc5PcnbpsMXT8cznf+HY4yxhesFAAA4pGzH7wheltmbvnwgyV9Nazg3ybOTPKuqrs7sdwDPmzY5L8ldptOfleSsrV4zAADAoWTH+hfZeGOM5yZ57l4nfzzJ8fu47L8mecxWrAsAAKCD7frzEQAAAGwTIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGa25e8IdnPDOS9carsjn/yzG7wSAAAAzwgCAAC0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoJkd270Aerr8FQ9feJtdP/n2TVgJAAD04xlBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNzBWCVfWeeU4DAADgwLdjrTOr6rZJbp/kiKo6PElNZ90hyVGbvDYAAAA2wXrPCP6PJFck+dbp3z0fb0vy8mU/aVXdqaouqKq/rqqrqup7q+rOVXVpVX1s+vfw6bJVVS+tqqur6sNVdd9lPy8AAADrhOAY4yVjjOOS/OwY45vGGMdNH985xlg6BJO8JMk7xxjfmuQ7k1yV5Kwk7xlj3CPJe6bjSXJikntMH2cmOWc/Pi8AAEB7a740dI8xxsuq6vuSHLtymzHGaxf9hFV1xyQPSPJj04wvJflSVZ2c5Aemi52f5H1Jnp3k5CSvHWOMJO+fnk282xjjukU/NwAAAHOGYFW9Lsk3J/lgkv+YTh5JFg7BJMcl2Z3k1VX1nZm91PTpSY5cEXfXJzlyOnxUkk+v2P6a6bRbhGBVnZnZM4b5hm/4hiWWBQAA0MNcIZhkV5J7T8/KbcTnvG+Sp40xLquql+TLLwNNkowxRlUt9LnGGOcmOTdJdu3atRHrBAAAOCTN+3cEr0zydRv0Oa9Jcs0Y47Lp+AWZheENVXW3JJn+vXE6/9okx6zY/ujpNAAAAJYwbwgekeSjVfWuqrp4z8cyn3CMcX2ST1fVPaeTTkjy0SQXJzl9Ou30zN6ZNNPpT5jePfR+ST7v9wMBAACWN+9LQ395gz/v05K8vqpuneTjSZ6YWZS+uarOSPKpJI+dLntJkpOSXJ3ki9NlAQAAWNK87xr6Rxv5SccYH8zs9w73dsI+LjuSPGUjPz8AAEBn875r6D9l9i6hSXLrJLdK8i9jjDts1sIAAADYHPM+I/i1ew5XVWX2t/3ut1mLAgAAYPPM+2YxNxszv5fkwZuwHgAAADbZvC8NPWXF0a/K7Pf7/nVTVgQAAMCmmvddQx++4vBNST6Z2ctDAQAAOMjM+zuC/mQDAADAIWKu3xGsqqOr6q1VdeP0cWFVHb3ZiwMAAGDjzftmMa9OcnGSr58+3j6dBgAAwEFm3hDcOcZ49RjjpunjNUl2buK6AAAA2CTzhuA/VtXjquqw6eNxSf5xMxcGAADA5pg3BJ+U5LFJrk9yXZJTk/zYJq0JAACATTTvn4/4lSSnjzE+myRVdeckL8wsEAEAADiIzPuM4HfsicAkGWN8Jsl3bc6SAAAA2EzzhuBXVdXhe45MzwjO+2wiAAAAB5B5Y+5FSf6sqt4yHX9Mkl/fnCUBAACwmeYKwTHGa6vq8iQPnE46ZYzx0c1bFgAAAJtl7pd3TuEn/gAAAA5y8/6OIAAAAIcIIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM9sWglV1WFX9ZVW9Yzp+XFVdVlVXV9WbqurW0+m3mY5fPZ1/7HatGQAA4FCwnc8IPj3JVSuOPz/J2WOMuyf5bJIzptPPSPLZ6fSzp8sBAACwpG0Jwao6OslDk7xyOl5JHpjkguki5yd55HT45Ol4pvNPmC4PAADAErbrGcEXJ/n5JP85Hb9Lks+NMW6ajl+T5Kjp8FFJPp0k0/mfny5/C1V1ZlVdXlWX7969ezPXDgAAcFDb8hCsqocluXGMccVGzh1jnDvG2DXG2LVz586NHA0AAHBI2bENn/P7kzyiqk5Kctskd0jykiR3qqod07N+Rye5drr8tUmOSXJNVe1Icsck/7j1ywYAADg0bPkzgmOM54wxjh5jHJvktCR/OMb40STvTXLqdLHTk7xtOnzxdDzT+X84xhhbuGQAAIBDyoH0dwSfneRZVXV1Zr8DeN50+nlJ7jKd/qwkZ23T+gAAAA4J2/HS0JuNMd6X5H3T4Y8nOX4fl/nXJI/Z0oUBAAAcwg6kZwQBAADYAkIQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhmx3YvAJb1J7/zsIW3uf9PvGMTVgIAAAcXzwgCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaGbHdi/gQLf7Fecutd3Onzxzg1cCAACwMTwjCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANLPlIVhVx1TVe6vqo1X1kap6+nT6navq0qr62PTv4dPpVVUvraqrq+rDVXXfrV4zAADAoWQ7nhG8KcnPjDHuneR+SZ5SVfdOclaS94wx7pHkPdPxJDkxyT2mjzOTnLP1SwYAADh0bHkIjjGuG2N8YDr8T0muSnJUkpOTnD9d7Pwkj5wOn5zktWPm/UnuVFV32+JlAwAAHDK29XcEq+rYJN+V5LIkR44xrpvOuj7JkdPho5J8esVm10yn7T3rzKq6vKou371796atGQAA4GC3bSFYVV+T5MIkzxhjfGHleWOMkWQsMm+Mce4YY9cYY9fOnTs3cKUAAACHlm0Jwaq6VWYR+PoxxkXTyTfsecnn9O+N0+nXJjlmxeZHT6cBAACwhO1419BKcl6Sq8YYv7nirIuTnD4dPj3J21ac/oTp3UPvl+TzK15CCgAAwIJ2bMPn/P4kj0/yV1X1wem0X0jyvCRvrqozknwqyWOn8y5JclKSq5N8MckTt3a5AAAAh5YtD8Exxp8mqVXOPmEflx9JnrKpiwIAAGhkW981FAAAgK0nBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgmR3bvQDYTpe+8qSFt/mhH79kE1YCAABbxzOCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhGCAIAADQjBAEAAJoRggAAAM0IQQAAgGaEIAAAQDNCEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmtmx3QuAg93bX3Xiwts8/El/sAkrAQCA+XhGEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmhCAAAEAzQhAAAKAZIQgAANCMEAQAAGhmx3YvAEje8uqHLLXdY574zg1eCQAAHXhGEAAAoBkhCAAA0IwQBAAAaEYIAgAANCMEAQAAmhGCAAAAzQhBAACAZoQgAABAM0IQAACgGSEIAADQjBAEAABoRggCAAA0IwQBAACaEYIAAADNCEEAAIBmdmz3AoCN8drXPHip7Z7wY+/a4JUAAHCg84wgAABAM0IQAACgGS8NBW72O69d7uWlP/GEL7+89GWvX27G037US1QBALaKEAQOSb/0locstd2vPeadG7wSAIADjxAEDjjPf+Nyzyo++zTPKgIAzOOgCcGqekiSlyQ5LMkrxxjP2+YlAcAB4eEXXLjUdm8/9dEbvBIADhYHRQhW1WFJfivJDyW5JslfVNXFY4yPbu/KgEPZT1203MtLf/uUL7+89MSLH7bUjD94xDtucfzE33vq4jMe+fJbHD/p935p4RmXPPLXFt5mKzz0onMW3ub3T3nyJqzk0PGIC96+1HYXn/rwmw+ffMFyz8q/7dTlXgVwMPjhiz6+8DZvOuWbNmElcHC44cUfWHibI59x301YyaHvoAjBJMcnuXqM8fEkqao3Jjk5iRAE2EInvXXxF2Nc8qizbnH8oRe9eOEZv3/KMxbeZj0PvfC8pbb7/UefcfPhh13wuqVmvOPUx6+Y8cYlZ5y21HYHukdd+L6ltnvro3/g5sOPvvD9S8248NH3u8Xxx1z44YVnvOXR37HU517LWW+9duFtnveoo25x/Oy3Xr/wjGc+6utucfz8i3YvPCNJTj9l582H33rBPyw141GnHnHz4Xe/cbkZDzrtyzP+5HXLfS33f/yXv5YrzrtxqRn/5Yy73nz4qnNuWGrGvZ585M2HP3X24tdtknzjM798/V7/gk8tNePrfu4bb3H8+hddtfiMn7nXUp97LTe85P8uvM2RT/++Wxy/8WV/uPCMuz7tgbec8fJLFp6RJHd96klfnvFbFy034ymnrHuZGmMsNXwrVdWpSR4yxvjx6fjjk3zPGOOpKy5zZpIzp6P3TPI364w9IslytyRmdJhxIK3FDDO6zDiQ1mKGGQfDjANpLWaYcTDMOJDWshUzvnGMsXPVc8cYB/xHklMz+73APccfn+Tl+znz8g1YlxmH6IwDaS1mmNFlxoG0FjPMOBhmHEhrMcOMg2HGgbSWA2HGwfIH5a9NcsyK40dPpwEAALCggyUE/yLJParquKq6dZLTkly8zWsCAAA4KB0UbxYzxripqp6a5F2Z/fmIV40xPrKfY8/d/5WZcQjP2Kg5ZphhxryJqo4AABA6SURBVNbPMcOMLjM2ao4ZZnSZsVFzDokZB8WbxQAAALBxDpaXhgIAALBBhCAAAEAz7UKwqh5SVX9TVVdX1Vnrb7HPGa+qqhur6sr9WMcxVfXeqvpoVX2kqp6+xIzbVtWfV9WHphn/az/Wc1hV/WVVvWPJ7T9ZVX9VVR+sqsuXnHGnqrqgqv66qq6qqu9dcPt7Tp9/z8cXqmrhv0JdVc+c/j+vrKo3VNVtl5jx9Gn7j8y7hn3tV1V156q6tKo+Nv17+JJzHjOt5T+rateSM14wXTcfrqq3VtWdlpjxq9P2H6yqd1fV1y86Y8V5P1NVo6qO2Ne266zjl6vq2hX7ykmLzphOf9r0f/KRqvqNJdbxphVr+GRVfXCJGfepqvfv+d6rquOXmPGdVfVn0/fw26vqDuvM2Oft1yL76xoz5t5X15gx93622owV56+7n62xjrn3s7XWMe9+tsY6Ft3PVpsz9762xoy597Va5T6uZm8cd1nN7sffVLM3kVt0xlOn7ee5DVltxutr9njiyun76lZLzDhvOu3DNbv/+5pFZ6w4/6VV9c9Lfi0PrKoPTF/L+VW17ntI1F6PGxa5XtaYMff1ssaMua+XNWa8pqo+seL75j5LzDhh+j/9YFX9aVXdfY4ZX/FYqha//97XjEXvv/c1Y9H771UfF9b899/7Wseit2df8diyFnxstcqMNa+XWvC+tqqeM+37f1NVD55OW+h2tKqOX/F/86GqetRaX1eSg+PvCG7UR2ZvNPN3Sb4pya2TfCjJvZeY84Ak901y5X6s5W5J7jsd/tokf7voWpJUkq+ZDt8qyWVJ7rfkep6V5H8neceS238yyRH7ef2cn+THp8O3TnKn/byur8/sD2kust1RST6R5HbT8Tcn+bEFZ3x7kiuT3D6zN2T6P0nuvsx+leQ3kpw1HT4ryfOXnHOvJPdM8r4ku5ac8aAkO6bDz19vLavMuMOKwz+d5BWLzphOPyazN4/61Hr73Srr+OUkP7vAdbqvGf99um5vMx2/6zJfy4rzX5Tkfy6xjncnOXE6fFKS9y0x4y+S/Lfp8JOS/Oo6M/Z5+7XI/rrGjLn31TVmzL2frTZjkf1sjXXMvZ+tMWPu/Wytr2XB/Wy1tcy9r60xY+59Lavcx2V2u3zadPorkjx5iRnfleTYzHHftcaMk6bzKskbllzHyn31NzN9/ywyYzq+K8nrkvzzEl/L9yX5dJJvmU7/lSRnzLHP3uJxwyLXyxoz5r5e1pgx9/WyxozXJDl1ns+/xoy/TXKv6fBPJXnNHDO+4uvO4vff+5qx6P33vmYsev+9z+swi91/r7kfZL7bs694bJkFH1utMmPN6yUL3Ndmdtv4oSS3SXJcZq1yWBa8Hc30uHM6fLckN+45vtpHt2cEj09y9Rjj42OMLyV5Y5KTFx0yxvjjJJ/Zn4WMMa4bY3xgOvxPSa7KLEIWmTHGGHt++ner6WPhd/+pqqOTPDTJKxfddqNU1R0z+6Y5L0nGGF8aY3xuP0aekOTvxhifWmLbHUluV7OfiN4+yf9bcPt7JblsjPHFMcZNSf4oySnrbbTKfnVyZjdAmf595DJzxhhXjTH+Zo61rzXj3dPXkyTvz+zveS464wsrjn511tlf1/heOzvJz6+3/Toz5rbKjCcned4Y49+my9y47DqqqpI8NrMHLYvOGEn2/FTxjllnf11lxrck+ePp8KVJHr3OjNVuv+beX1ebsci+usaMufezdW6L59rPNuj2fLUZc+9n661jgf1stTlz72trzJh7X1vjPu6BSS6YTl9vP9vnjDHGX44xPrnadnPOuGQ6byT586xxm7jGjC8kN183t8va++o+Z1TVYUlekNm+uszX8h9JvjTG+Nvp9HVvA/Z+3DCtf+7rZV8zpvXNfb2sMWPu62W1GYtaZcZCt82rWfT+e5UZC91/rzJjofvvNcx9/72WeW7P1nhsOfd91Woz1rteFryvPTnJG8cY/zbG+ESSq5Mcv+jt6IrHnUly28zxf9wtBI/K7Kdee1yTBe+sN0NVHZvZT8EuW2Lbw6anxW9McukYY+EZSV6c2Tflfy6x7R4jybur6oqqOnOJ7Y9LsjvJq6eXVryyqr56P9ZzWtZ5sLMvY4xrk7wwyd8nuS7J58cY715wzJVJ7l9Vd6mq22f208ljFl3L5MgxxnXT4euTHLnknI32pCR/sMyGVfXrVfXpJD+a5H8usf3JSa4dY3xomc+/wlOnl7m8ar2XhaziWzK7ni+rqj+qqu/ej7XcP8kNY4yPLbHtM5K8YPo/fWGS5ywx4yP58g/FHpMF9te9br+W2l/35zZwtRnL7GcrZyy7n+3ja1l4P9trxlL72Sr/pwvvZ3vNWWpf22vGQvva3vdxmf2k/HMrHuysez++EfeTa82o2UsPH5/kncvMqKpXZ/b98q1JXrbEjKcmuXjF995CX0tmsbRjxcvbTs36twF7P264Sxa8XvYxYxmrzpj3elljxq9P37tnV9Vtlpjx40kuqaprpnU8b50Zyf4/lppnxjz33/ucseDt6lfMWOJ2da2vZZ7bs9UeWy5yX7WRj09Xu/1bt0/mvR2tqu+pqo8k+askP7nie3KfuoXgAadmvw9wYZJn7PXTlrmMMf5jjHGfzH66c3xVffuCn/9hSW4cY1yx6Ofey38dY9w3yYlJnlJVD1hw+x2ZPYV+zhjju5L8S2ZP1y+sZr+X8Igkb1li28Mz++Y6LsnXJ/nqqnrcIjPGGFdl9tKLd2d2B/TBzH7iul+mn27u10/QNkJV/WKSm5K8fpntxxi/OMY4Ztr+qQt+7tsn+YUsEZB7OSfJNye5T2bB/6IlZuxIcufMXt71c0nePP2Echk/kiV+cDF5cpJnTv+nz8z0U8sFPSnJT1XVFZm9/ORL82y01u3XvPvr/t4GrjZj0f1s5YzM9u+F97N9rGPh/WwfMxbez9b4P11oP9vHnIX3tX3MWGhf2/s+LrNYWsj+3k/OMeO3k/zxGONPlpkxxnhiZvc3VyX54QVnPCCzB4JrBuRaM5J8W2Y/PD27qv48yT9ljfusjXjcsEUz1r1e1pjxnMz2te/O7Pvv2UvMeGaSk8YYRyd5dWYv/V3P/j6WWnPGAvff+5yx4O3qvmYseru61v/HPLdn6z62nOO+asMen2bj7mtXnTPGuGyM8W2Z7bvPqXXe56JbCF6bW/6U6+jptG0x/bTqwiSvH2NctD+zpqe635vkIQtu+v1JHlFVn8zspbIPrKrfXeLzXzv9e2OSt2Z257KIa5Jcs+KnrBdk9o23jBOTfGCMccMS2/5gkk+MMXaPMf49yf9v725C7rjKAI7/n1IabNFWsaWBovEjutNCQCJUGxu/FtIYcdHShaUouGrdSgTrQjcurOtahFqVIpYYKShoN1LapjY2aWqTElG0YFIqFdGCtc3j4pxLhmRm7py5rwY6/x+8vPcdZh7OzDzv+bj3zLkPUZ6faJKZ92Xmrsz8KPAyZV73HGciYjtA/T06/fB/LSJuBz4D3FYrz038kDXTj3q8hzJIP1pz9jrgSERc2xIkM8/UztBZ4F7a8xVKzj5UZyAdprwTPGlxg64oU5A/Bzw4owwAX6DkKZQ3P5rPJTNPZOYnM3MXpWH9w7pjBuqvpnzdijpwQoy1edYToznP+srRmmcD59KUZ0PXozXPBuI05drANWnOtXrcqo37MHBVnFvMZHI7vkE7ORgjIr4OXE15PmxWjLrtdUobPKlO7MT4GPBe4FTN1csj4lRrOTLzscz8SGZ+iDLlbKzNuqDfAHyXtvuyFX2PwRgN96U3RpYpeZllSvb3Gc/1vhgPAx/s9GkeZEJfYgv6UoMxWtrvCeVYW6/2xLiRxnp15Fym1mdDfcuWtmrL+qcj9d/g+GRuPZrlQ4l/UtatGLS0geCTwM4oK1tdRnkH7NDFKEh9R/c+4LnMnPIuUV+Mq6Ou/BQRbwI+AZxoiZGZX83M6zJzB+V6PJKZTZ+ARcQVEfHm1WvKQ8lNK6pm5mngLxHx/rppL/D7lhgdm3y68mdgd0RcXu/RXsq7tE0i4pr6+x2UyupHM8tziNL5ov7+2cw4G4uIT1Omvtycma/MjLGz8+c+2vP1mcy8JjN31Jx9gfIg9enGcmzv/LmfxnytDlI6YUTE+ygPkL80I87HgROZ+cKMY6E8d3JjfX0T0Dy9tJOvlwBfoyz0MLb/UP01OV+3qA7sjdGSZ30xWvNspByT82zkekzOszXXdHKejcSZnGsj12Ryrg20cc9RBi+fr7uty7ON28mhGBHxReBTwK11sN8a42TUlSTr9bp5rGwDMZ7KzGs7ufpKZg6uTjlyLqv7so3y6dfgfRnoN9xGw33Zir7HUIyW+zISYzVICMrzY4P/u30xKPXOlfV/Fs7l7qCt6EsNxWhpv0ditNSrfTGebKxXx67HpPpspG85ua3ayv7pSP13CLglIrZFxLuAncDh1nq0jm8ura/fSflU+0+jhcqGFZHeCD+U57Wep4yeD8yM8WPKNJ//UBJ57epaPTFuoHwUfYwydfBpyhSClhgfAH5XYxxnzcpJE+LtYcaqoZRVWI/Wn2c3uK7XA7+t53MQeOuMGFcAfwOu3OA6fINSwR2nrMK2bUaM31AqiqPA3rl5RXnu4teUDtevgLfNjLO/vv43cAb45YwYpyhz2Ff5um7FsL4YP63X9Rjwc8rCHrP/15i24l9fOX5AmT9/jFIBb58R4zLggXo+R4Cb5pwLZXW6L2+QIzcAT9VcewLYNSPGXZR68XnKcyyxJkZv/dWSryMxJufqSIzJeTYUoyXPRsoxOc9GYkzOs7FzacyzobJMzrWRGJNzjYE2jtLmHKbUST9hpI4eiXFnzbPXKAPc782I8RqlL7E6v8E2uC8G5c34R2uOHKd8yvKW1nKct8+6VUOHzuXblIHKScr0s7V5Uo/bw7lVMiffl5EYk+/LSIzJ92UkxiOd+/IAdaXVxhj7a4yjlJUl373m2N6+FG114lCMye33SIyWenVtv5D19epgDNrqswv6ljT2rQZijN4XGtta4EDN25OcW525qR6lPIv6bN3vCPDZdddndaAkSZIkaSGWNjVUkiRJkhbPgaAkSZIkLYwDQUmSJElaGAeCkiRJkrQwDgQlSZIkaWEcCEqSJEnSwlx6sQsgSdLFEhF3A7sp3z0GpV18PDPv7uxzO3AH8I/OoX/NzC919tlD+S62M519Xge+07c9M/d1jt0BPAz8sbPP2ylfTn3B9szcPfkEJUka4EBQkrR0t2Tm3wEi4irgKz373JmZT6/+iIh7evb5ZmYe7NlnaHvXvZl5T88+Q9slSdqIU0MlSZIkaWEcCEqSJEnSwjgQlCRJkqSFcSAoSZIkSQvjQFCSJEmSFsaBoCRJkiQtjF8fIUlasheB+yPibP37EuAX5+3zMvCtiHi1s+3Yefv8CzhQv3Nw5fTI9q5XgX31uwhXzo5slyRpY5GZF7sMkiRJkqT/I6eGSpIkSdLCOBCUJEmSpIVxIChJkiRJC+NAUJIkSZIWxoGgJEmSJC3MfwF8OY0vt3geKgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1080x1080 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "ax = plt.subplots(figsize=(15, 15))\n",
        "ax = sns.countplot(x='1번', data=plt_word_num)\n",
        "ax.set_xlabel('행 당 단어 개수')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2TwVyGYp9DUV"
      },
      "source": [
        "# 시 하나 당 단어 수 세보자"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-VwDhxTn9Cvs",
        "outputId": "5cc75c44-1a66-4ccc-dbb5-d016d0a9db0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "시 하나 당 단어 수: 170.23972602739727\n"
          ]
        }
      ],
      "source": [
        "for i in range(0, len(df_poem)):\n",
        "  word = []\n",
        "  poem = df_poem.iloc[i] # 시 하나 들고오기\n",
        "  word = poem.split()# 그 시를 단어로 split\n",
        "  word_num += len(word) # 시의 전체 단어 개수 센다. \n",
        "print('시 하나 당 단어 수:', word_num/len(df_poem))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tDiuUkiFs3sG"
      },
      "source": [
        "# 인풋 벡터 형태로 변환 작업 수행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7hagexebi6V-"
      },
      "outputs": [],
      "source": [
        "p_list = []\n",
        "\n",
        "for i in range(0, len(df_poem)):\n",
        "  poem = df_poem.iloc[i] # 시 하나 들고오기\n",
        "  # line = poem.splitlines() # 그 시를 행 기준으로 나눠서 리스트로 저장\n",
        "  p_list += poem\n",
        "\n",
        "\n",
        "data = tf.ragged.constant(p_list)\n",
        "text_ds = tf.data.Dataset.from_tensor_slices(p_list)\n",
        "\n",
        "batch_size = 128\n",
        "text_ds = text_ds.shuffle(buffer_size=256)\n",
        "text_ds = text_ds.batch(batch_size)\n",
        "\n",
        "vocab_size = 200000\n",
        "maxlen = 15\n",
        "\n",
        "vectorize_layer = TextVectorization(\n",
        "    max_tokens=vocab_size - 1,\n",
        "    output_mode = 'int',\n",
        "    output_sequence_length = maxlen + 1\n",
        "    )\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sY0r7zvA32Do"
      },
      "outputs": [],
      "source": [
        "def prepare_lm_inputs_labels(text):\n",
        "    \"\"\"\n",
        "    Shift word sequences by 1 position so that the target for position (i) is\n",
        "    word at position (i+1). The model will use all words up till position (i)\n",
        "    to predict the next word.\n",
        "    \"\"\"\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4A5GXX9n5XhT"
      },
      "outputs": [],
      "source": [
        "text_ds = text_ds.map(prepare_lm_inputs_labels)\n",
        "text_ds = text_ds.prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiLbREKJ5jYv"
      },
      "source": [
        "# gpt decoder 부분 하나씩 만들기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPq_f0mB5ZkN"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
        "        super(TokenAndPositionEmbedding, self).__init__()\n",
        "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
        "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
        "\n",
        "    def call(self, x):\n",
        "        maxlen = tf.shape(x)[-1]\n",
        "        # token 위치에 따른 embedding을 하기위함\n",
        "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
        "        positions = self.pos_emb(positions)\n",
        "        x = self.token_emb(x)\n",
        "        return x + positions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDkheWcJ-ZCh"
      },
      "source": [
        "# transformer 시작"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WphFO8n85m0q"
      },
      "outputs": [],
      "source": [
        "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
        "    \"\"\"\n",
        "    Mask the upper half of the dot product matrix in self attention.\n",
        "    This prevents flow of information from future tokens to current token.\n",
        "    1's in the lower triangle, counting from the lower right corner.\n",
        "    \"\"\"\n",
        "    i = tf.range(n_dest)[:, None]\n",
        "    j = tf.range(n_src)\n",
        "    m = i >= j - n_src + n_dest\n",
        "    mask = tf.cast(m, dtype)\n",
        "    mask = tf.reshape(mask, [1, n_dest, n_src])\n",
        "    mult = tf.concat(\n",
        "        [tf.expand_dims(batch_size, -1), tf.constant([1, 1], dtype=tf.int32)], 0\n",
        "    )\n",
        "    return tf.tile(mask, mult)\n",
        "\n",
        "\n",
        "class TransformerBlock(layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.ffn = keras.Sequential(\n",
        "            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = layers.Dropout(rate)\n",
        "        self.dropout2 = layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        input_shape = tf.shape(inputs)\n",
        "        batch_size = input_shape[0]\n",
        "        seq_len = input_shape[1]\n",
        "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, tf.bool)\n",
        "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
        "        attention_output = self.dropout1(attention_output)\n",
        "        out1 = self.layernorm1(inputs + attention_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output)\n",
        "        return self.layernorm2(out1 + ffn_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hbvgf5c75pvo"
      },
      "outputs": [],
      "source": [
        "# 변수 정의 및 모델 함수 정의\n",
        "\n",
        "embed_dim = 256  # Embedding size for each token\n",
        "num_heads = 2  # Number of attention heads\n",
        "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
        "\n",
        "def create_model():\n",
        "    inputs = layers.Input(shape=(maxlen,), dtype=tf.int32) \n",
        "    # input 정의\n",
        "    \n",
        "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim) \n",
        "    # token Embedding + positional Embedding layer class 정의\n",
        "    \n",
        "    x = embedding_layer(inputs) \n",
        "    # 선언한 Embedding layer class 이용해 Embedding\n",
        "    \n",
        "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim) \n",
        "    # transformer block layer class 정의\n",
        "    \n",
        "    x = transformer_block(x) \n",
        "    # 선언한 transformer layer class 이용해 학습\n",
        "    \n",
        "    outputs = layers.Dense(vocab_size)(x) \n",
        "    # 압축된 결과를 vocab에 맞춰 팽창 (후보단어 선별을 위한 각 단어에 대한 결과치 도출)\n",
        "    \n",
        "    model = keras.Model(inputs=inputs, outputs=[outputs, x])  \n",
        "    # model 정의\n",
        "    \n",
        "    loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) \n",
        "    model.compile(\n",
        "        \"adam\", loss=[loss_fn, None],\n",
        "    )  # No loss and optimization based on word embeddings from transformer block\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TW6CxjRZ6B4i"
      },
      "outputs": [],
      "source": [
        "class TextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model.\n",
        "    1. Feed some starting prompt to the model\n",
        "    2. Predict probabilities for the next token\n",
        "    3. Sample the next token and add it to the next input\n",
        "\n",
        "    Arguments:\n",
        "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
        "        start_tokens: List of integers, the token indices for the starting prompt.\n",
        "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
        "        top_k: Integer, sample from the `top_k` token predictions.\n",
        "        print_every: Integer, print after this many epochs.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
        "    ):\n",
        "        self.max_tokens = max_tokens\n",
        "        self.start_tokens = start_tokens\n",
        "        self.index_to_word = index_to_word\n",
        "        self.print_every = print_every\n",
        "        self.k = top_k\n",
        "\n",
        "    def sample_from(self, logits):\n",
        "        logits, indices = tf.math.top_k(logits, k=self.k, sorted=True)\n",
        "        indices = np.asarray(indices).astype(\"int32\")\n",
        "        preds = keras.activations.softmax(tf.expand_dims(logits, 0))[0]\n",
        "        preds = np.asarray(preds).astype(\"float32\")\n",
        "        return np.random.choice(indices, p=preds)\n",
        "\n",
        "    def detokenize(self, number):\n",
        "        return self.index_to_word[number]\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        start_tokens = [_ for _ in self.start_tokens]\n",
        "        if (epoch + 1) % self.print_every != 0:\n",
        "            return\n",
        "        num_tokens_generated = 0\n",
        "        tokens_generated = []\n",
        "        while num_tokens_generated <= self.max_tokens:\n",
        "            pad_len = maxlen - len(start_tokens)\n",
        "            sample_index = len(start_tokens) - 1\n",
        "            if pad_len < 0:\n",
        "                x = start_tokens[:maxlen]\n",
        "                sample_index = maxlen - 1\n",
        "            elif pad_len > 0:\n",
        "                x = start_tokens + [0] * pad_len\n",
        "            else:\n",
        "                x = start_tokens\n",
        "            x = np.array([x])\n",
        "            y, _ = self.model.predict(x)\n",
        "            sample_token = self.sample_from(y[0][sample_index])\n",
        "            tokens_generated.append(sample_token)\n",
        "            start_tokens.append(sample_token)\n",
        "            num_tokens_generated = len(tokens_generated)\n",
        "        txt = \" \".join(\n",
        "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
        "        )\n",
        "        print(f\"generated text:\\n{txt}\\n\")\n",
        "\n",
        "\n",
        "# Tokenize starting prompt\n",
        "word_to_index = {}\n",
        "for index, word in enumerate(vocab):\n",
        "    word_to_index[word] = index\n",
        "\n",
        "start_prompt = \"벚꽃\"\n",
        "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
        "num_tokens_generated = 40\n",
        "text_gen_callback = TextGenerator(num_tokens_generated, start_tokens, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0k9tUPDD6D_P",
        "outputId": "c7113128-f8e1-4195-f435-ee94d531a61e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_10\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_11 (InputLayer)       [(None, 15)]              0         \n",
            "                                                                 \n",
            " token_and_position_embeddin  (None, 15, 256)          51203840  \n",
            " g_10 (TokenAndPositionEmbed                                     \n",
            " ding)                                                           \n",
            "                                                                 \n",
            " transformer_block_10 (Trans  (None, 15, 256)          658688    \n",
            " formerBlock)                                                    \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 15, 200000)        51400000  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 103,262,528\n",
            "Trainable params: 103,262,528\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "  6/748 [..............................] - ETA: 2:05 - loss: 11.2738 - dense_32_loss: 11.2738"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0798s vs `on_train_batch_end` time: 0.0884s). Check your callbacks.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "748/748 [==============================] - ETA: 0s - loss: 0.2535 - dense_32_loss: 0.2535generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 128s 168ms/step - loss: 0.2535 - dense_32_loss: 0.2535\n",
            "Epoch 2/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 2.0241e-04 - dense_32_loss: 2.0241e-04generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 2.0241e-04 - dense_32_loss: 2.0241e-04\n",
            "Epoch 3/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 8.5720e-05 - dense_32_loss: 8.5720e-05generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 169ms/step - loss: 8.5720e-05 - dense_32_loss: 8.5720e-05\n",
            "Epoch 4/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 4.5025e-05 - dense_32_loss: 4.5025e-05generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 169ms/step - loss: 4.5025e-05 - dense_32_loss: 4.5025e-05\n",
            "Epoch 5/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 2.6395e-05 - dense_32_loss: 2.6395e-05generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 169ms/step - loss: 2.6395e-05 - dense_32_loss: 2.6395e-05\n",
            "Epoch 6/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 1.6387e-05 - dense_32_loss: 1.6387e-05generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 1.6387e-05 - dense_32_loss: 1.6387e-05\n",
            "Epoch 7/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 1.0915e-05 - dense_32_loss: 1.0915e-05generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 1.0915e-05 - dense_32_loss: 1.0915e-05\n",
            "Epoch 8/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 6.3948e-06 - dense_32_loss: 6.3948e-06generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 6.3948e-06 - dense_32_loss: 6.3948e-06\n",
            "Epoch 9/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 5.5221e-06 - dense_32_loss: 5.5221e-06generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 5.5221e-06 - dense_32_loss: 5.5221e-06\n",
            "Epoch 10/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 3.1035e-06 - dense_32_loss: 3.1035e-06generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 3.1035e-06 - dense_32_loss: 3.1035e-06\n",
            "Epoch 11/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 6.2937e-07 - dense_32_loss: 6.2937e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 6.2937e-07 - dense_32_loss: 6.2937e-07\n",
            "Epoch 12/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 4.0106e-07 - dense_32_loss: 4.0106e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 4.0106e-07 - dense_32_loss: 4.0106e-07\n",
            "Epoch 13/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 3.2810e-07 - dense_32_loss: 3.2810e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 3.2810e-07 - dense_32_loss: 3.2810e-07\n",
            "Epoch 14/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 2.6284e-07 - dense_32_loss: 2.6284e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 2.6284e-07 - dense_32_loss: 2.6284e-07\n",
            "Epoch 15/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 2.0629e-07 - dense_32_loss: 2.0629e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 2.0629e-07 - dense_32_loss: 2.0629e-07\n",
            "Epoch 16/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 1.5639e-07 - dense_32_loss: 1.5639e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 1.5639e-07 - dense_32_loss: 1.5639e-07\n",
            "Epoch 17/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 1.1420e-07 - dense_32_loss: 1.1420e-07generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 1.1420e-07 - dense_32_loss: 1.1420e-07\n",
            "Epoch 18/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 8.0831e-08 - dense_32_loss: 8.0831e-08generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 168ms/step - loss: 8.0831e-08 - dense_32_loss: 8.0831e-08\n",
            "Epoch 19/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 5.4507e-08 - dense_32_loss: 5.4507e-08generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 169ms/step - loss: 5.4507e-08 - dense_32_loss: 5.4507e-08\n",
            "Epoch 20/20\n",
            "748/748 [==============================] - ETA: 0s - loss: 3.2773e-08 - dense_32_loss: 3.2773e-08generated text:\n",
            "[UNK]                                         \n",
            "\n",
            "748/748 [==============================] - 126s 169ms/step - loss: 3.2773e-08 - dense_32_loss: 3.2773e-08\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe8c8f309d0>"
            ]
          },
          "execution_count": 239,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = create_model()\n",
        "model.fit(text_ds, verbose=1, epochs=20, callbacks=[text_gen_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sFgxtMXm6RER"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "mini-gpt_test.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNyztOA/sRKQ01gNbRPnoeI",
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}